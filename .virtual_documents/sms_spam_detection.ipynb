import numpy as np
import pandas as pd


df=pd.read_csv("spam.csv", encoding="latin1")


df.head()


#1.Data cleaning
#2.EDA
#Text processing
#MOdel building
#evaluatain
#improvement
#website
#deploy





df.info()


#drop last 3 columns
df.drop(columns=["Unnamed: 2","Unnamed: 3","Unnamed: 4"],inplace=True)


df.rename(columns={"v1":"target","v2":"text"},inplace=True)


from sklearn.preprocessing import LabelEncoder 
encoder=LabelEncoder()
df["target"]=encoder.fit_transform(df["target"])


df.head()


#has to chk missing value 
df.isnull().sum()


#has to chl duplicate value(important)
df.duplicated().sum()


# remove duplicate (important)
df=df.drop_duplicates(keep="first")


df.duplicated().sum()





df["target"].value_counts()


import matplotlib.pyplot as plt
plt.pie(df["target"].value_counts(),labels=["ham","spam"],autopct="%0.2f")
plt.show()
#slightly data imbalance


import nltk


nltk.download("punkt")
nltk.download('punkt_tab')


df["num_characters"]=df["text"].apply(len)


df["num_words"]=df["text"].apply(lambda x:len(nltk.word_tokenize(x)))


df["num_sentences"]=df["text"].apply(lambda x: len(nltk.sent_tokenize(x)))


df.head()


df[["num_characters","num_words","num_sentences"]].describe()


#ham
df[df["target"]==0][["num_characters","num_words","num_sentences"]].describe()


#spam
df[df["target"]==1][["num_characters","num_words","num_sentences"]].describe()


import seaborn as sns


sns.histplot(df[df["target"]==0]["num_characters"])
sns.histplot(df[df["target"]==1]["num_characters"],color=("red"))


sns.histplot(df[df["target"]==0]["num_words"])
sns.histplot(df[df["target"]==1]["num_words"],color=("red"))





import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords
import string
from nltk.stem.porter import PorterStemmer #yeee similar words koo hata deta hai aur sirf ek word hi rakhega (read,reads,reading) me see sirf read rakhega aur sbb hata dega


ps=PorterStemmer()



def transform_text(text):
    # 1. Lowercase
    text = text.lower()

    # 2. Tokenization
    text = nltk.word_tokenize(text)   # tumne words_tokenization likha tha, galat hai

    y = []
    # 3. Remove non-alphanumeric
    for i in text:
        if i.isalnum():   # sirf alphabets aur numbers allow
            y.append(i)

    text = y[:]   # copy banao
    y.clear()

    # 4. Remove stopwords and punctuation
    for i in text:
        if i not in stopwords.words("english") and i not in string.punctuation:
            y.append(i)

    text = y[:]   # copy again
    y.clear()

    # 5. Apply stemming
    for i in text:
        y.append(ps.stem(i))

    # 6. Join back into string
    return " ".join(y)


df["transformed_text"]=df["text"].apply(transform_text)


df.head()


#create a wordcloud (WordCloud ek visualization hai jo text ke sabse frequent......
#(baar-baar aane wale) words ko bade aur prominent size me dikhata hai.)
from wordcloud import WordCloud
wc=WordCloud(width=500,height=500,min_font_size=10,background_color="white")



spam_wc = wc.generate(df[df["target"]==1]["transformed_text"].str.cat(sep=" "))


plt.imshow(spam_wc)


spam_wc = wc.generate(df[df["target"]==0]["transformed_text"].str.cat(sep=" "))
plt.imshow(spam_wc)


#custom code
spam_corpus=[]
for msg in df[df["target"]==1]["transformed_text"].tolist():
   for word in msg.split():
       spam_corpus.append(word)


len(spam_corpus)


#Ye kisi bhi iterable (list, tuple, string, etc.) me elements ki frequency count karta hai.
#Output ek dictionary jaisa hota hai jisme â†’ element : count.
from collections import Counter
sns.barplot (x= pd.DataFrame(Counter(spam_corpus).most_common(30))[0],
             y=pd.DataFrame(Counter(spam_corpus).most_common(30))[1],
             palette="coolwarm")
plt.xticks(rotation ="vertical")
plt.show()


ham_corpus=[]
for msg in df[df["target"]==0]["transformed_text"].tolist():
   for word in msg.split():
       ham_corpus.append(word)


len(ham_corpus)


from collections import Counter
sns.barplot (x= pd.DataFrame(Counter(ham_corpus).most_common(30))[0],
             y=pd.DataFrame(Counter(ham_corpus).most_common(30))[1],
             palette="coolwarm")
plt.xticks(rotation ="vertical")
plt.show()





from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv=CountVectorizer()
tfidf=TfidfVectorizer(max_features=3000)
X=tfidf.fit_transform(df["transformed_text"]).toarray()


X.shape



y=df["target"].values


y


from sklearn.model_selection import train_test_split


X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)


from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score



gnb=GaussianNB()
mnb=MultinomialNB()
bnb=BernoulliNB()


gnb.fit(X_train,y_train)
y_pred1 = gnb.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1)) 
print(precision_score(y_test,y_pred1))


mnb.fit(X_train,y_train)
y_pred1 = mnb.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1)) 
print(precision_score(y_test,y_pred1))


bnb.fit(X_train,y_train)
y_pred1 = bnb.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1)) 
print(precision_score(y_test,y_pred1))


#choose tfidf--------mnb fn


import pickle
pickle.dump(tfidf,open("vectorizer.pkl","wb"))
pickle.dump(mnb,open("model.pkl","wb"))



